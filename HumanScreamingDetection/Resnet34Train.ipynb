{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:58:24.212285700Z",
     "start_time": "2023-10-13T07:58:24.209283900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset ImageFolder\n    Number of datapoints: 3544\n    Root location: Data/Images\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 862), interpolation=bilinear, max_size=None, antialias=warn)\n               ToTensor()\n           )"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'Data/Images' #looking in subfolder train\n",
    "\n",
    "scream_dataset = datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=transforms.Compose([transforms.Resize((64,862)),\n",
    "                                  transforms.ToTensor()])\n",
    ")\n",
    "scream_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:58:25.112220Z",
     "start_time": "2023-10-13T07:58:25.015925800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class category and index of the images: {'not': 0, 'scream': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_map=scream_dataset.class_to_idx\n",
    "\n",
    "print(\"\\nClass category and index of the images: {}\\n\".format(class_map))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:58:26.171221500Z",
     "start_time": "2023-10-13T07:58:26.162239100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 2835\n",
      "Testing size: 709\n"
     ]
    }
   ],
   "source": [
    "#split data to test and train\n",
    "#use 80% to train\n",
    "train_size = int(0.8 * len(scream_dataset))\n",
    "test_size = len(scream_dataset) - train_size\n",
    "scream_train_dataset, scream_test_dataset = torch.utils.data.random_split(scream_dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Training size:\", len(scream_train_dataset))\n",
    "print(\"Testing size:\",len(scream_test_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:58:27.359020700Z",
     "start_time": "2023-10-13T07:58:27.347130Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "Counter({1: 753, 0: 2082})"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# labels in training set\n",
    "train_classes = [label for _, label in scream_train_dataset]\n",
    "Counter(train_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:58:37.265486700Z",
     "start_time": "2023-10-13T07:58:29.468598900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    scream_train_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    scream_test_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:58:40.433633Z",
     "start_time": "2023-10-13T07:58:40.352667200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.3922, 0.1686, 0.1333,  ..., 0.2392, 0.2314, 0.1882],\n         [0.3098, 0.1333, 0.1216,  ..., 0.1608, 0.2000, 0.1725],\n         [0.1843, 0.1176, 0.1176,  ..., 0.1725, 0.1373, 0.2392],\n         ...,\n         [0.1725, 0.2745, 0.2784,  ..., 0.2784, 0.2784, 0.2745],\n         [0.1373, 0.2706, 0.2824,  ..., 0.2784, 0.2784, 0.2784],\n         [0.1294, 0.2784, 0.2824,  ..., 0.2824, 0.2824, 0.2824]],\n\n        [[0.7961, 0.6941, 0.6549,  ..., 0.7333, 0.7294, 0.7059],\n         [0.7647, 0.6549, 0.6353,  ..., 0.6863, 0.7137, 0.6941],\n         [0.7020, 0.6118, 0.6078,  ..., 0.6941, 0.6588, 0.7333],\n         ...,\n         [0.4431, 0.1843, 0.1490,  ..., 0.1686, 0.1725, 0.1843],\n         [0.5373, 0.2078, 0.1373,  ..., 0.1647, 0.1647, 0.1490],\n         [0.5490, 0.1569, 0.1176,  ..., 0.1373, 0.1255, 0.1176]],\n\n        [[0.3647, 0.4902, 0.5176,  ..., 0.4549, 0.4588, 0.4784],\n         [0.4118, 0.5176, 0.5255,  ..., 0.4980, 0.4745, 0.4902],\n         [0.4824, 0.5373, 0.5373,  ..., 0.4902, 0.5137, 0.4549],\n         ...,\n         [0.5569, 0.4863, 0.4627,  ..., 0.4784, 0.4824, 0.4863],\n         [0.5529, 0.5020, 0.4549,  ..., 0.4745, 0.4745, 0.4627],\n         [0.5529, 0.4706, 0.4392,  ..., 0.4549, 0.4431, 0.4392]]])"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = train_dataloader.dataset[0][0]\n",
    "td"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:58:41.568696Z",
     "start_time": "2023-10-13T07:58:41.536698900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 64, 862])"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:58:43.050524700Z",
     "start_time": "2023-10-13T07:58:43.033398500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:58:44.190751600Z",
     "start_time": "2023-10-13T07:58:44.187753600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34\n",
    "import torch\n",
    "\n",
    "model = resnet34()\n",
    "model.fc = nn.Linear(512,2)\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T08:31:02.792640600Z",
     "start_time": "2023-10-13T08:31:02.505329500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "# Create the training function\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # Counter for correct predictions\n",
    "    total = 0  # Counter for total examples\n",
    "\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = pred.max(1)\n",
    "        total += Y.size(0)\n",
    "        correct += predicted.eq(Y).sum().item()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]  Train Accuracy: {(100 * correct / total):.2f}%')\n",
    "\n",
    "\n",
    "# Create the validation/test function\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "\n",
    "    print(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')\n",
    "    return test_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T08:32:36.046277600Z",
     "start_time": "2023-10-13T08:32:36.043136300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.759905  [    0/ 2835]  Train Accuracy: 60.94%\n",
      "loss: 0.482047  [  640/ 2835]  Train Accuracy: 68.47%\n",
      "loss: 0.422999  [ 1280/ 2835]  Train Accuracy: 74.48%\n",
      "loss: 0.474069  [ 1920/ 2835]  Train Accuracy: 76.06%\n",
      "loss: 0.457741  [ 2560/ 2835]  Train Accuracy: 77.13%\n",
      "\n",
      "Test Error:\n",
      "acc: 83.1%, avg loss: 0.006937\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.428111  [    0/ 2835]  Train Accuracy: 82.81%\n",
      "loss: 0.255079  [  640/ 2835]  Train Accuracy: 82.81%\n",
      "loss: 0.669328  [ 1280/ 2835]  Train Accuracy: 82.51%\n",
      "loss: 0.436707  [ 1920/ 2835]  Train Accuracy: 82.46%\n",
      "loss: 0.482669  [ 2560/ 2835]  Train Accuracy: 82.01%\n",
      "\n",
      "Test Error:\n",
      "acc: 78.8%, avg loss: 0.008527\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.400275  [    0/ 2835]  Train Accuracy: 82.81%\n",
      "loss: 0.315782  [  640/ 2835]  Train Accuracy: 84.38%\n",
      "loss: 0.334474  [ 1280/ 2835]  Train Accuracy: 84.90%\n",
      "loss: 0.391821  [ 1920/ 2835]  Train Accuracy: 84.68%\n",
      "loss: 0.280403  [ 2560/ 2835]  Train Accuracy: 84.83%\n",
      "\n",
      "Test Error:\n",
      "acc: 84.2%, avg loss: 0.005831\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.338641  [    0/ 2835]  Train Accuracy: 87.50%\n",
      "loss: 0.402949  [  640/ 2835]  Train Accuracy: 85.09%\n",
      "loss: 0.268721  [ 1280/ 2835]  Train Accuracy: 86.01%\n",
      "loss: 0.242171  [ 1920/ 2835]  Train Accuracy: 85.89%\n",
      "loss: 0.438653  [ 2560/ 2835]  Train Accuracy: 85.21%\n",
      "\n",
      "Test Error:\n",
      "acc: 82.9%, avg loss: 0.005993\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.376454  [    0/ 2835]  Train Accuracy: 82.81%\n",
      "loss: 0.411533  [  640/ 2835]  Train Accuracy: 86.08%\n",
      "loss: 0.481289  [ 1280/ 2835]  Train Accuracy: 85.94%\n",
      "loss: 0.329803  [ 1920/ 2835]  Train Accuracy: 85.74%\n",
      "loss: 0.309350  [ 2560/ 2835]  Train Accuracy: 85.82%\n",
      "\n",
      "Test Error:\n",
      "acc: 85.2%, avg loss: 0.006245\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.390646  [    0/ 2835]  Train Accuracy: 79.69%\n",
      "loss: 0.321319  [  640/ 2835]  Train Accuracy: 82.10%\n",
      "loss: 0.344508  [ 1280/ 2835]  Train Accuracy: 85.04%\n",
      "loss: 0.413564  [ 1920/ 2835]  Train Accuracy: 85.33%\n",
      "loss: 0.430229  [ 2560/ 2835]  Train Accuracy: 85.56%\n",
      "\n",
      "Test Error:\n",
      "acc: 85.6%, avg loss: 0.005834\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.311121  [    0/ 2835]  Train Accuracy: 82.81%\n",
      "loss: 0.416237  [  640/ 2835]  Train Accuracy: 84.66%\n",
      "loss: 0.311539  [ 1280/ 2835]  Train Accuracy: 85.86%\n",
      "loss: 0.355892  [ 1920/ 2835]  Train Accuracy: 86.29%\n",
      "loss: 0.292136  [ 2560/ 2835]  Train Accuracy: 86.66%\n",
      "\n",
      "Test Error:\n",
      "acc: 87.9%, avg loss: 0.004946\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.327031  [    0/ 2835]  Train Accuracy: 85.94%\n",
      "loss: 0.511296  [  640/ 2835]  Train Accuracy: 85.51%\n",
      "loss: 0.397852  [ 1280/ 2835]  Train Accuracy: 86.31%\n",
      "loss: 0.396112  [ 1920/ 2835]  Train Accuracy: 86.90%\n",
      "loss: 0.236587  [ 2560/ 2835]  Train Accuracy: 86.66%\n",
      "\n",
      "Test Error:\n",
      "acc: 87.3%, avg loss: 0.005830\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.260075  [    0/ 2835]  Train Accuracy: 89.06%\n",
      "loss: 0.284302  [  640/ 2835]  Train Accuracy: 86.93%\n",
      "loss: 0.214591  [ 1280/ 2835]  Train Accuracy: 87.43%\n",
      "loss: 0.321613  [ 1920/ 2835]  Train Accuracy: 87.15%\n",
      "loss: 0.213186  [ 2560/ 2835]  Train Accuracy: 87.50%\n",
      "\n",
      "Test Error:\n",
      "acc: 86.2%, avg loss: 0.005364\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.227228  [    0/ 2835]  Train Accuracy: 90.62%\n",
      "loss: 0.309963  [  640/ 2835]  Train Accuracy: 90.34%\n",
      "loss: 0.399171  [ 1280/ 2835]  Train Accuracy: 89.29%\n",
      "loss: 0.284729  [ 1920/ 2835]  Train Accuracy: 89.21%\n",
      "loss: 0.211049  [ 2560/ 2835]  Train Accuracy: 88.91%\n",
      "\n",
      "Test Error:\n",
      "acc: 77.2%, avg loss: 0.021757\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.154274  [    0/ 2835]  Train Accuracy: 96.88%\n",
      "loss: 0.281517  [  640/ 2835]  Train Accuracy: 88.92%\n",
      "loss: 0.168388  [ 1280/ 2835]  Train Accuracy: 89.51%\n",
      "loss: 0.291594  [ 1920/ 2835]  Train Accuracy: 89.16%\n",
      "loss: 0.195824  [ 2560/ 2835]  Train Accuracy: 88.68%\n",
      "\n",
      "Test Error:\n",
      "acc: 87.9%, avg loss: 0.005225\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.197387  [    0/ 2835]  Train Accuracy: 93.75%\n",
      "loss: 0.307301  [  640/ 2835]  Train Accuracy: 89.91%\n",
      "loss: 0.256727  [ 1280/ 2835]  Train Accuracy: 89.96%\n",
      "loss: 0.326787  [ 1920/ 2835]  Train Accuracy: 89.92%\n",
      "loss: 0.352817  [ 2560/ 2835]  Train Accuracy: 89.60%\n",
      "\n",
      "Test Error:\n",
      "acc: 74.5%, avg loss: 0.011725\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.282506  [    0/ 2835]  Train Accuracy: 85.94%\n",
      "loss: 0.228549  [  640/ 2835]  Train Accuracy: 89.77%\n",
      "loss: 0.353493  [ 1280/ 2835]  Train Accuracy: 89.66%\n",
      "loss: 0.304746  [ 1920/ 2835]  Train Accuracy: 88.81%\n",
      "loss: 0.383027  [ 2560/ 2835]  Train Accuracy: 88.99%\n",
      "\n",
      "Test Error:\n",
      "acc: 86.3%, avg loss: 0.007246\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.197966  [    0/ 2835]  Train Accuracy: 92.19%\n",
      "loss: 0.301000  [  640/ 2835]  Train Accuracy: 89.91%\n",
      "loss: 0.241788  [ 1280/ 2835]  Train Accuracy: 91.00%\n",
      "loss: 0.245043  [ 1920/ 2835]  Train Accuracy: 90.83%\n",
      "loss: 0.367852  [ 2560/ 2835]  Train Accuracy: 90.51%\n",
      "\n",
      "Test Error:\n",
      "acc: 81.1%, avg loss: 0.007479\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.209337  [    0/ 2835]  Train Accuracy: 93.75%\n",
      "loss: 0.266204  [  640/ 2835]  Train Accuracy: 89.91%\n",
      "loss: 0.167252  [ 1280/ 2835]  Train Accuracy: 89.88%\n",
      "loss: 0.174850  [ 1920/ 2835]  Train Accuracy: 90.37%\n",
      "loss: 0.233662  [ 2560/ 2835]  Train Accuracy: 90.36%\n",
      "\n",
      "Test Error:\n",
      "acc: 66.1%, avg loss: 0.016352\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.253191  [    0/ 2835]  Train Accuracy: 92.19%\n",
      "loss: 0.147340  [  640/ 2835]  Train Accuracy: 91.90%\n",
      "loss: 0.270595  [ 1280/ 2835]  Train Accuracy: 91.00%\n",
      "loss: 0.322816  [ 1920/ 2835]  Train Accuracy: 91.08%\n",
      "loss: 0.205166  [ 2560/ 2835]  Train Accuracy: 90.51%\n",
      "\n",
      "Test Error:\n",
      "acc: 86.6%, avg loss: 0.013598\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.115873  [    0/ 2835]  Train Accuracy: 96.88%\n",
      "loss: 0.317886  [  640/ 2835]  Train Accuracy: 91.05%\n",
      "loss: 0.372147  [ 1280/ 2835]  Train Accuracy: 90.10%\n",
      "loss: 0.312655  [ 1920/ 2835]  Train Accuracy: 89.77%\n",
      "loss: 0.176446  [ 2560/ 2835]  Train Accuracy: 89.98%\n",
      "\n",
      "Test Error:\n",
      "acc: 88.2%, avg loss: 0.005854\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.256608  [    0/ 2835]  Train Accuracy: 89.06%\n",
      "loss: 0.153953  [  640/ 2835]  Train Accuracy: 91.76%\n",
      "loss: 0.284581  [ 1280/ 2835]  Train Accuracy: 90.92%\n",
      "loss: 0.171118  [ 1920/ 2835]  Train Accuracy: 90.68%\n",
      "loss: 0.247520  [ 2560/ 2835]  Train Accuracy: 90.47%\n",
      "\n",
      "Test Error:\n",
      "acc: 86.6%, avg loss: 0.006975\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.236599  [    0/ 2835]  Train Accuracy: 93.75%\n",
      "loss: 0.291617  [  640/ 2835]  Train Accuracy: 92.33%\n",
      "loss: 0.189576  [ 1280/ 2835]  Train Accuracy: 91.96%\n",
      "loss: 0.173907  [ 1920/ 2835]  Train Accuracy: 91.43%\n",
      "loss: 0.299688  [ 2560/ 2835]  Train Accuracy: 91.58%\n",
      "\n",
      "Test Error:\n",
      "acc: 78.7%, avg loss: 0.013653\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.193250  [    0/ 2835]  Train Accuracy: 89.06%\n",
      "loss: 0.131035  [  640/ 2835]  Train Accuracy: 91.90%\n",
      "loss: 0.135960  [ 1280/ 2835]  Train Accuracy: 91.67%\n",
      "loss: 0.183894  [ 1920/ 2835]  Train Accuracy: 92.19%\n",
      "loss: 0.176806  [ 2560/ 2835]  Train Accuracy: 91.88%\n",
      "\n",
      "Test Error:\n",
      "acc: 85.5%, avg loss: 0.009942\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.208411  [    0/ 2835]  Train Accuracy: 90.62%\n",
      "loss: 0.179488  [  640/ 2835]  Train Accuracy: 93.04%\n",
      "loss: 0.121353  [ 1280/ 2835]  Train Accuracy: 93.01%\n",
      "loss: 0.280143  [ 1920/ 2835]  Train Accuracy: 92.59%\n",
      "loss: 0.221906  [ 2560/ 2835]  Train Accuracy: 92.49%\n",
      "\n",
      "Test Error:\n",
      "acc: 86.9%, avg loss: 0.007844\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.127319  [    0/ 2835]  Train Accuracy: 96.88%\n",
      "loss: 0.142030  [  640/ 2835]  Train Accuracy: 93.18%\n",
      "loss: 0.180027  [ 1280/ 2835]  Train Accuracy: 93.08%\n",
      "loss: 0.200255  [ 1920/ 2835]  Train Accuracy: 92.49%\n",
      "loss: 0.183797  [ 2560/ 2835]  Train Accuracy: 92.57%\n",
      "\n",
      "Test Error:\n",
      "acc: 86.7%, avg loss: 0.007335\n",
      "\n",
      "Early stopping triggered. No improvement in test loss for 15 epochs.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Define the early stopping parameters\n",
    "early_stopping_patience = 15  # Number of epochs to wait before early stopping\n",
    "best_loss = torch.inf\n",
    "wait = 0  # Counter for patience\n",
    "epochs = 150\n",
    "\n",
    "best_model_weights = None\n",
    "\n",
    "# Training loop\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t + 1}\\n-------------------------------')\n",
    "    train(train_dataloader, model, cost, optimizer)\n",
    "    test_loss = test(test_dataloader, model)\n",
    "\n",
    "    # Check if the test loss has improved\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        wait = 0  # Reset patience\n",
    "\n",
    "        # Save the best model weights\n",
    "        best_model_weights = model.state_dict()\n",
    "    else:\n",
    "        wait += 1\n",
    "\n",
    "    if wait >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered. No improvement in test loss for {} epochs.\".format(early_stopping_patience))\n",
    "        break  # Stop training\n",
    "\n",
    "# Restore the best model weights\n",
    "if best_model_weights is not None:\n",
    "    model.load_state_dict(best_model_weights)\n",
    "\n",
    "print('Done!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T08:40:44.244865900Z",
     "start_time": "2023-10-13T08:32:39.335335200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as Resnet34_Model_2023-10-13--16-42-18.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current timestamp in the desired format\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "# Define the file name with the timestamp\n",
    "file_name = f\"Resnet34_Model_{timestamp}.pt\"\n",
    "\n",
    "# Save the entire model (including architecture and weights)\n",
    "torch.save(model, file_name)\n",
    "\n",
    "# Print the saved file name\n",
    "print(f\"Model saved as {file_name}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T08:42:19.099100200Z",
     "start_time": "2023-10-13T08:42:18.930419300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "# Load the model's state_dict\n",
    "model = torch.load('Resnet34_Model_2023-10-13--14-03-45.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T08:42:22.571652900Z",
     "start_time": "2023-10-13T08:42:22.369613Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 64, 32, 431]           9,408\n",
      "       BatchNorm2d-2          [-1, 64, 32, 431]             128\n",
      "              ReLU-3          [-1, 64, 32, 431]               0\n",
      "         MaxPool2d-4          [-1, 64, 16, 216]               0\n",
      "            Conv2d-5          [-1, 64, 16, 216]          36,864\n",
      "       BatchNorm2d-6          [-1, 64, 16, 216]             128\n",
      "              ReLU-7          [-1, 64, 16, 216]               0\n",
      "            Conv2d-8          [-1, 64, 16, 216]          36,864\n",
      "       BatchNorm2d-9          [-1, 64, 16, 216]             128\n",
      "             ReLU-10          [-1, 64, 16, 216]               0\n",
      "       BasicBlock-11          [-1, 64, 16, 216]               0\n",
      "           Conv2d-12          [-1, 64, 16, 216]          36,864\n",
      "      BatchNorm2d-13          [-1, 64, 16, 216]             128\n",
      "             ReLU-14          [-1, 64, 16, 216]               0\n",
      "           Conv2d-15          [-1, 64, 16, 216]          36,864\n",
      "      BatchNorm2d-16          [-1, 64, 16, 216]             128\n",
      "             ReLU-17          [-1, 64, 16, 216]               0\n",
      "       BasicBlock-18          [-1, 64, 16, 216]               0\n",
      "           Conv2d-19          [-1, 64, 16, 216]          36,864\n",
      "      BatchNorm2d-20          [-1, 64, 16, 216]             128\n",
      "             ReLU-21          [-1, 64, 16, 216]               0\n",
      "           Conv2d-22          [-1, 64, 16, 216]          36,864\n",
      "      BatchNorm2d-23          [-1, 64, 16, 216]             128\n",
      "             ReLU-24          [-1, 64, 16, 216]               0\n",
      "       BasicBlock-25          [-1, 64, 16, 216]               0\n",
      "           Conv2d-26          [-1, 128, 8, 108]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 8, 108]             256\n",
      "             ReLU-28          [-1, 128, 8, 108]               0\n",
      "           Conv2d-29          [-1, 128, 8, 108]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 8, 108]             256\n",
      "           Conv2d-31          [-1, 128, 8, 108]           8,192\n",
      "      BatchNorm2d-32          [-1, 128, 8, 108]             256\n",
      "             ReLU-33          [-1, 128, 8, 108]               0\n",
      "       BasicBlock-34          [-1, 128, 8, 108]               0\n",
      "           Conv2d-35          [-1, 128, 8, 108]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 8, 108]             256\n",
      "             ReLU-37          [-1, 128, 8, 108]               0\n",
      "           Conv2d-38          [-1, 128, 8, 108]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 8, 108]             256\n",
      "             ReLU-40          [-1, 128, 8, 108]               0\n",
      "       BasicBlock-41          [-1, 128, 8, 108]               0\n",
      "           Conv2d-42          [-1, 128, 8, 108]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 8, 108]             256\n",
      "             ReLU-44          [-1, 128, 8, 108]               0\n",
      "           Conv2d-45          [-1, 128, 8, 108]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 8, 108]             256\n",
      "             ReLU-47          [-1, 128, 8, 108]               0\n",
      "       BasicBlock-48          [-1, 128, 8, 108]               0\n",
      "           Conv2d-49          [-1, 128, 8, 108]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 8, 108]             256\n",
      "             ReLU-51          [-1, 128, 8, 108]               0\n",
      "           Conv2d-52          [-1, 128, 8, 108]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 8, 108]             256\n",
      "             ReLU-54          [-1, 128, 8, 108]               0\n",
      "       BasicBlock-55          [-1, 128, 8, 108]               0\n",
      "           Conv2d-56           [-1, 256, 4, 54]         294,912\n",
      "      BatchNorm2d-57           [-1, 256, 4, 54]             512\n",
      "             ReLU-58           [-1, 256, 4, 54]               0\n",
      "           Conv2d-59           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-60           [-1, 256, 4, 54]             512\n",
      "           Conv2d-61           [-1, 256, 4, 54]          32,768\n",
      "      BatchNorm2d-62           [-1, 256, 4, 54]             512\n",
      "             ReLU-63           [-1, 256, 4, 54]               0\n",
      "       BasicBlock-64           [-1, 256, 4, 54]               0\n",
      "           Conv2d-65           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-66           [-1, 256, 4, 54]             512\n",
      "             ReLU-67           [-1, 256, 4, 54]               0\n",
      "           Conv2d-68           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-69           [-1, 256, 4, 54]             512\n",
      "             ReLU-70           [-1, 256, 4, 54]               0\n",
      "       BasicBlock-71           [-1, 256, 4, 54]               0\n",
      "           Conv2d-72           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-73           [-1, 256, 4, 54]             512\n",
      "             ReLU-74           [-1, 256, 4, 54]               0\n",
      "           Conv2d-75           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-76           [-1, 256, 4, 54]             512\n",
      "             ReLU-77           [-1, 256, 4, 54]               0\n",
      "       BasicBlock-78           [-1, 256, 4, 54]               0\n",
      "           Conv2d-79           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-80           [-1, 256, 4, 54]             512\n",
      "             ReLU-81           [-1, 256, 4, 54]               0\n",
      "           Conv2d-82           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-83           [-1, 256, 4, 54]             512\n",
      "             ReLU-84           [-1, 256, 4, 54]               0\n",
      "       BasicBlock-85           [-1, 256, 4, 54]               0\n",
      "           Conv2d-86           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-87           [-1, 256, 4, 54]             512\n",
      "             ReLU-88           [-1, 256, 4, 54]               0\n",
      "           Conv2d-89           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-90           [-1, 256, 4, 54]             512\n",
      "             ReLU-91           [-1, 256, 4, 54]               0\n",
      "       BasicBlock-92           [-1, 256, 4, 54]               0\n",
      "           Conv2d-93           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-94           [-1, 256, 4, 54]             512\n",
      "             ReLU-95           [-1, 256, 4, 54]               0\n",
      "           Conv2d-96           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-97           [-1, 256, 4, 54]             512\n",
      "             ReLU-98           [-1, 256, 4, 54]               0\n",
      "       BasicBlock-99           [-1, 256, 4, 54]               0\n",
      "          Conv2d-100           [-1, 512, 2, 27]       1,179,648\n",
      "     BatchNorm2d-101           [-1, 512, 2, 27]           1,024\n",
      "            ReLU-102           [-1, 512, 2, 27]               0\n",
      "          Conv2d-103           [-1, 512, 2, 27]       2,359,296\n",
      "     BatchNorm2d-104           [-1, 512, 2, 27]           1,024\n",
      "          Conv2d-105           [-1, 512, 2, 27]         131,072\n",
      "     BatchNorm2d-106           [-1, 512, 2, 27]           1,024\n",
      "            ReLU-107           [-1, 512, 2, 27]               0\n",
      "      BasicBlock-108           [-1, 512, 2, 27]               0\n",
      "          Conv2d-109           [-1, 512, 2, 27]       2,359,296\n",
      "     BatchNorm2d-110           [-1, 512, 2, 27]           1,024\n",
      "            ReLU-111           [-1, 512, 2, 27]               0\n",
      "          Conv2d-112           [-1, 512, 2, 27]       2,359,296\n",
      "     BatchNorm2d-113           [-1, 512, 2, 27]           1,024\n",
      "            ReLU-114           [-1, 512, 2, 27]               0\n",
      "      BasicBlock-115           [-1, 512, 2, 27]               0\n",
      "          Conv2d-116           [-1, 512, 2, 27]       2,359,296\n",
      "     BatchNorm2d-117           [-1, 512, 2, 27]           1,024\n",
      "            ReLU-118           [-1, 512, 2, 27]               0\n",
      "          Conv2d-119           [-1, 512, 2, 27]       2,359,296\n",
      "     BatchNorm2d-120           [-1, 512, 2, 27]           1,024\n",
      "            ReLU-121           [-1, 512, 2, 27]               0\n",
      "      BasicBlock-122           [-1, 512, 2, 27]               0\n",
      "AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n",
      "          Linear-124                    [-1, 2]           1,026\n",
      "================================================================\n",
      "Total params: 21,285,698\n",
      "Trainable params: 21,285,698\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.63\n",
      "Forward/backward pass size (MB): 106.06\n",
      "Params size (MB): 81.20\n",
      "Estimated Total Size (MB): 187.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Assuming 'model' is your PyTorch model\n",
    "summary(model, input_size=(3, 64, 862))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T08:42:25.550329400Z",
     "start_time": "2023-10-13T08:42:25.517066900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pad_waveform(waveform, target_length):\n",
    "    num_channels, current_length = waveform.shape\n",
    "\n",
    "    if current_length < target_length:\n",
    "        # Calculate the amount of padding needed\n",
    "        padding = target_length - current_length\n",
    "        # Pad the waveform with zeros on the right side\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "\n",
    "    return waveform\n",
    "\n",
    "# Define a function to transform audio data into images\n",
    "def transform_data_to_image(audio, sample_rate, label, i):\n",
    "    # Pad waveform to a consistent length of 44100 samples\n",
    "    audio = pad_waveform(audio, 441000)\n",
    "\n",
    "    spectrogram_tensor = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=64, n_fft=1024)(audio)[0] + 1e-10\n",
    "\n",
    "    # Save the spectrogram as an image\n",
    "    image_path = f'Data/TestImages/{label}/image{i}.png'\n",
    "\n",
    "    plt.imsave(image_path, spectrogram_tensor.log2().numpy(), cmap='viridis')\n",
    "    return image_path\n",
    "\n",
    "# Define the image transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 862)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x[:3, :, :])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T08:42:28.493130800Z",
     "start_time": "2023-10-13T08:42:28.488134500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "                Filename  Prediction\n0    ---1_cCGK4M_out.wav           0\n1    -20uudT97E0_out.wav           1\n2    -2yygHLdpXc_out.wav           1\n3    -3bGlOhRkAo_out.wav           1\n4    -4pUrlMafww_out.wav           1\n..                   ...         ...\n857  _QMEw67gWIA_out.wav           1\n858  _TLzbbay6Hw_out.wav           1\n859  _XPPISqmXSE_out.wav           1\n860  _xRpsu02t9o_out.wav           1\n861  _zzhHu7HwZc_out.wav           1\n\n[862 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>---1_cCGK4M_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-20uudT97E0_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-2yygHLdpXc_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-3bGlOhRkAo_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-4pUrlMafww_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>857</th>\n      <td>_QMEw67gWIA_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>858</th>\n      <td>_TLzbbay6Hw_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>859</th>\n      <td>_XPPISqmXSE_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>_xRpsu02t9o_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>861</th>\n      <td>_zzhHu7HwZc_out.wav</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>862 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing WAV files\n",
    "folder_path = 'Data/Screaming'  # Replace with the path to your folder\n",
    "label = 'Screaming'  # Label for the images\n",
    "\n",
    "# Create an empty list to store data\n",
    "predictions_data = []\n",
    "\n",
    "# Iterate through WAV files in the folder\n",
    "for i, filename in enumerate(os.listdir(folder_path)):\n",
    "    if filename.endswith('.wav'):\n",
    "        # Load the audio\n",
    "        audio, sample_rate = torchaudio.load(os.path.join(folder_path, filename))\n",
    "\n",
    "        # Transform audio to an image and save it\n",
    "        image_path = transform_data_to_image(audio, sample_rate, label, i)\n",
    "\n",
    "        # Load the saved image and apply transformations\n",
    "        image = Image.open(image_path)\n",
    "        image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Make predictions using the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image.to(device))\n",
    "\n",
    "        predict = outputs.argmax(dim=1).cpu().detach().numpy().ravel()[0]\n",
    "\n",
    "        # Store the filename and prediction in the DataFrame\n",
    "        predictions_data.append({'Filename': filename, 'Prediction': predict})\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "scream_predictions_df = pd.DataFrame(predictions_data)\n",
    "\n",
    "# Display the DataFrame with predictions\n",
    "scream_predictions_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T08:43:28.765781100Z",
     "start_time": "2023-10-13T08:42:31.491342200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "Prediction\n1    748\n0    114\nName: count, dtype: int64"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scream_predictions_df['Prediction'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T08:43:35.995725Z",
     "start_time": "2023-10-13T08:43:35.983207200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "                 Filename  Prediction\n0     --PJHxphWEs_out.wav           0\n1     -28U1_qW0sU_out.wav           0\n2     -4xJv59_zcA_out.wav           0\n3     -5GhUbDLYkQ_out.wav           0\n4     -5Jlimvsuwo_out.wav           0\n...                   ...         ...\n2626  _XusTa2prSw_out.wav           0\n2627  _y07ENAx2_E_out.wav           0\n2628  _yqlQimkHpQ_out.wav           0\n2629  _Zsk5Fxqbkc_out.wav           0\n2630  __qxgIqI0uA_out.wav           0\n\n[2631 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>--PJHxphWEs_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-28U1_qW0sU_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-4xJv59_zcA_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-5GhUbDLYkQ_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-5Jlimvsuwo_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2626</th>\n      <td>_XusTa2prSw_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2627</th>\n      <td>_y07ENAx2_E_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2628</th>\n      <td>_yqlQimkHpQ_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2629</th>\n      <td>_Zsk5Fxqbkc_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2630</th>\n      <td>__qxgIqI0uA_out.wav</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2631 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the folder containing WAV files\n",
    "folder_path = 'Data/NotScreaming'  # Replace with the path to your folder\n",
    "label = 'NotScreaming'  # Label for the images\n",
    "import pandas as pd\n",
    "\n",
    "# Create an empty list to store data\n",
    "predictions_data = []\n",
    "\n",
    "# Iterate through WAV files in the folder\n",
    "for i, filename in enumerate(os.listdir(folder_path)):\n",
    "    if filename.endswith('.wav'):\n",
    "        # Load the audio\n",
    "        audio, sample_rate = torchaudio.load(os.path.join(folder_path, filename))\n",
    "\n",
    "        # Transform audio to an image and save it\n",
    "        image_path = transform_data_to_image(audio, sample_rate, label, i)\n",
    "\n",
    "        # Load the saved image and apply transformations\n",
    "        image = Image.open(image_path)\n",
    "        image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Make predictions using the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image.to(device))\n",
    "\n",
    "        predict = outputs.argmax(dim=1).cpu().detach().numpy().ravel()[0]\n",
    "\n",
    "        # Store the filename and prediction in the DataFrame\n",
    "        predictions_data.append({'Filename': filename, 'Prediction': predict})\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "not_scream_predictions_df = pd.DataFrame(predictions_data)\n",
    "\n",
    "# Display the DataFrame with predictions\n",
    "not_scream_predictions_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T08:46:42.241695Z",
     "start_time": "2023-10-13T08:43:39.027376500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "Prediction\n0    2547\n1      84\nName: count, dtype: int64"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_scream_predictions_df['Prediction'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T08:46:55.522587500Z",
     "start_time": "2023-10-13T08:46:55.508626900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "# Compute TP, FP, TN, FN for Scream predictions\n",
    "scream_TP = scream_predictions_df['Prediction'].value_counts().get(1, 0)\n",
    "scream_FN = scream_predictions_df['Prediction'].value_counts().get(0, 0)\n",
    "scream_samples = len(scream_predictions_df)\n",
    "\n",
    "# Compute TP, FP, TN, FN for Not Scream predictions\n",
    "scream_TN = not_scream_predictions_df['Prediction'].value_counts().get(0, 0)\n",
    "scream_FP = not_scream_predictions_df['Prediction'].value_counts().get(1, 0)\n",
    "not_scream_samples = len(not_scream_predictions_df)\n",
    "\n",
    "# Calculate total samples\n",
    "total_samples = scream_samples + not_scream_samples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T08:46:58.666118100Z",
     "start_time": "2023-10-13T08:46:58.629264800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n",
      "Precision: 0.90\n",
      "Recall: 0.87\n",
      "F1 Score: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = (scream_TP + scream_TN) / total_samples\n",
    "\n",
    "# Calculate precision\n",
    "precision = scream_TP / (scream_TP + scream_FP)\n",
    "\n",
    "# Calculate recall\n",
    "recall = scream_TP / (scream_TP + scream_FN)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 Score: {:.2f}\".format(f1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T08:46:58.668117800Z",
     "start_time": "2023-10-13T08:46:58.666118100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
