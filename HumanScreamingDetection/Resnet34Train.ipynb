{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:41:38.681145400Z",
     "start_time": "2023-10-13T06:41:38.666630600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset ImageFolder\n    Number of datapoints: 3295\n    Root location: Data/Images\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 862), interpolation=bilinear, max_size=None, antialias=warn)\n               ToTensor()\n           )"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'Data/Images' #looking in subfolder train\n",
    "\n",
    "scream_dataset = datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=transforms.Compose([transforms.Resize((64,862)),\n",
    "                                  transforms.ToTensor()])\n",
    ")\n",
    "scream_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:41:39.261510900Z",
     "start_time": "2023-10-13T06:41:39.236255Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class category and index of the images: {'not': 0, 'scream': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_map=scream_dataset.class_to_idx\n",
    "\n",
    "print(\"\\nClass category and index of the images: {}\\n\".format(class_map))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:41:40.580383600Z",
     "start_time": "2023-10-13T06:41:40.576756100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 2636\n",
      "Testing size: 659\n"
     ]
    }
   ],
   "source": [
    "#split data to test and train\n",
    "#use 80% to train\n",
    "train_size = int(0.8 * len(scream_dataset))\n",
    "test_size = len(scream_dataset) - train_size\n",
    "scream_train_dataset, scream_test_dataset = torch.utils.data.random_split(scream_dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Training size:\", len(scream_train_dataset))\n",
    "print(\"Testing size:\",len(scream_test_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:41:46.518616800Z",
     "start_time": "2023-10-13T06:41:46.502619200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Counter({0: 1742, 1: 894})"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# labels in training set\n",
    "train_classes = [label for _, label in scream_train_dataset]\n",
    "Counter(train_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:41:54.794803300Z",
     "start_time": "2023-10-13T06:41:47.678978300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    scream_train_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    scream_test_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:41:57.305849100Z",
     "start_time": "2023-10-13T06:41:57.302601300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.1255, 0.1255, 0.1176,  ..., 0.5255, 0.4392, 0.6549],\n         [0.1294, 0.1176, 0.1176,  ..., 0.5137, 0.4196, 0.6549],\n         [0.2510, 0.2000, 0.1569,  ..., 0.4863, 0.5255, 0.5725],\n         ...,\n         [0.2784, 0.2784, 0.2784,  ..., 0.2784, 0.2784, 0.1608],\n         [0.2784, 0.2784, 0.2824,  ..., 0.2784, 0.2824, 0.1608],\n         [0.2784, 0.2745, 0.2745,  ..., 0.2784, 0.2784, 0.1569]],\n\n        [[0.5686, 0.5686, 0.5922,  ..., 0.8314, 0.8078, 0.8588],\n         [0.6549, 0.6157, 0.6000,  ..., 0.8275, 0.8039, 0.8588],\n         [0.7412, 0.7137, 0.6824,  ..., 0.8235, 0.8314, 0.8431],\n         ...,\n         [0.1569, 0.0824, 0.0784,  ..., 0.1529, 0.1490, 0.4745],\n         [0.1686, 0.0667, 0.1020,  ..., 0.1529, 0.1255, 0.4745],\n         [0.1647, 0.0471, 0.0471,  ..., 0.0863, 0.0863, 0.4784]],\n\n        [[0.5490, 0.5490, 0.5412,  ..., 0.2863, 0.3373, 0.2000],\n         [0.5176, 0.5333, 0.5412,  ..., 0.2941, 0.3490, 0.2000],\n         [0.4471, 0.4745, 0.4980,  ..., 0.3098, 0.2863, 0.2549],\n         ...,\n         [0.4706, 0.4039, 0.4000,  ..., 0.4667, 0.4627, 0.5569],\n         [0.4784, 0.3882, 0.4235,  ..., 0.4667, 0.4431, 0.5569],\n         [0.4745, 0.3725, 0.3725,  ..., 0.4118, 0.4118, 0.5569]]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = train_dataloader.dataset[0][0]\n",
    "td"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:41:58.554871800Z",
     "start_time": "2023-10-13T06:41:58.523308200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 64, 862])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:41:59.659742200Z",
     "start_time": "2023-10-13T06:41:59.656228500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:42:02.250257Z",
     "start_time": "2023-10-13T06:42:01.046568900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34\n",
    "import torch\n",
    "\n",
    "model = resnet34()\n",
    "model.fc = nn.Linear(512,2)\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:42:05.806192700Z",
     "start_time": "2023-10-13T06:42:04.860048700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.00001)\n",
    "\n",
    "# Create the training function\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # Counter for correct predictions\n",
    "    total = 0  # Counter for total examples\n",
    "\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = pred.max(1)\n",
    "        total += Y.size(0)\n",
    "        correct += predicted.eq(Y).sum().item()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]  Train Accuracy: {(100 * correct / total):.2f}%')\n",
    "\n",
    "\n",
    "# Create the validation/test function\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "\n",
    "    print(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')\n",
    "    return test_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:57:03.357107700Z",
     "start_time": "2023-10-13T06:57:03.353595Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.071783  [    0/ 2636]  Train Accuracy: 96.88%\n",
      "loss: 0.165275  [  640/ 2636]  Train Accuracy: 97.73%\n",
      "loss: 0.051002  [ 1280/ 2636]  Train Accuracy: 97.62%\n",
      "loss: 0.105276  [ 1920/ 2636]  Train Accuracy: 97.53%\n",
      "loss: 0.101107  [ 2560/ 2636]  Train Accuracy: 97.52%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.0%, avg loss: 0.001517\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.161570  [    0/ 2636]  Train Accuracy: 96.88%\n",
      "loss: 0.034444  [  640/ 2636]  Train Accuracy: 97.87%\n",
      "loss: 0.064294  [ 1280/ 2636]  Train Accuracy: 98.36%\n",
      "loss: 0.026478  [ 1920/ 2636]  Train Accuracy: 98.64%\n",
      "loss: 0.064906  [ 2560/ 2636]  Train Accuracy: 98.48%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.0%, avg loss: 0.001462\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.019630  [    0/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.036792  [  640/ 2636]  Train Accuracy: 99.15%\n",
      "loss: 0.024843  [ 1280/ 2636]  Train Accuracy: 98.96%\n",
      "loss: 0.014494  [ 1920/ 2636]  Train Accuracy: 99.04%\n",
      "loss: 0.012554  [ 2560/ 2636]  Train Accuracy: 98.86%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.0%, avg loss: 0.001493\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.012205  [    0/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.070093  [  640/ 2636]  Train Accuracy: 99.29%\n",
      "loss: 0.042092  [ 1280/ 2636]  Train Accuracy: 98.96%\n",
      "loss: 0.009469  [ 1920/ 2636]  Train Accuracy: 99.09%\n",
      "loss: 0.015362  [ 2560/ 2636]  Train Accuracy: 99.16%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.3%, avg loss: 0.001546\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.016368  [    0/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.040144  [  640/ 2636]  Train Accuracy: 99.29%\n",
      "loss: 0.046375  [ 1280/ 2636]  Train Accuracy: 99.33%\n",
      "loss: 0.008304  [ 1920/ 2636]  Train Accuracy: 99.50%\n",
      "loss: 0.010278  [ 2560/ 2636]  Train Accuracy: 99.54%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.2%, avg loss: 0.001657\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.020366  [    0/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.012598  [  640/ 2636]  Train Accuracy: 99.72%\n",
      "loss: 0.025090  [ 1280/ 2636]  Train Accuracy: 99.55%\n",
      "loss: 0.007077  [ 1920/ 2636]  Train Accuracy: 99.45%\n",
      "loss: 0.026380  [ 2560/ 2636]  Train Accuracy: 99.35%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.2%, avg loss: 0.001572\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.006895  [    0/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.007272  [  640/ 2636]  Train Accuracy: 99.57%\n",
      "loss: 0.008973  [ 1280/ 2636]  Train Accuracy: 99.55%\n",
      "loss: 0.007550  [ 1920/ 2636]  Train Accuracy: 99.50%\n",
      "loss: 0.008996  [ 2560/ 2636]  Train Accuracy: 99.43%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.2%, avg loss: 0.001360\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.019070  [    0/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.084783  [  640/ 2636]  Train Accuracy: 99.15%\n",
      "loss: 0.041114  [ 1280/ 2636]  Train Accuracy: 99.33%\n",
      "loss: 0.002652  [ 1920/ 2636]  Train Accuracy: 99.29%\n",
      "loss: 0.021396  [ 2560/ 2636]  Train Accuracy: 99.47%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.3%, avg loss: 0.001586\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.004265  [    0/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.005316  [  640/ 2636]  Train Accuracy: 98.72%\n",
      "loss: 0.010562  [ 1280/ 2636]  Train Accuracy: 99.26%\n",
      "loss: 0.009747  [ 1920/ 2636]  Train Accuracy: 99.45%\n",
      "loss: 0.003955  [ 2560/ 2636]  Train Accuracy: 99.50%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.2%, avg loss: 0.001602\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.003478  [    0/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.002272  [  640/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.004677  [ 1280/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.002958  [ 1920/ 2636]  Train Accuracy: 99.95%\n",
      "loss: 0.002295  [ 2560/ 2636]  Train Accuracy: 99.92%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.2%, avg loss: 0.001575\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.003051  [    0/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.001850  [  640/ 2636]  Train Accuracy: 99.86%\n",
      "loss: 0.006039  [ 1280/ 2636]  Train Accuracy: 99.93%\n",
      "loss: 0.002134  [ 1920/ 2636]  Train Accuracy: 99.95%\n",
      "loss: 0.003365  [ 2560/ 2636]  Train Accuracy: 99.96%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.3%, avg loss: 0.002473\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.056787  [    0/ 2636]  Train Accuracy: 98.44%\n",
      "loss: 0.002617  [  640/ 2636]  Train Accuracy: 99.86%\n",
      "loss: 0.003567  [ 1280/ 2636]  Train Accuracy: 99.85%\n",
      "loss: 0.001658  [ 1920/ 2636]  Train Accuracy: 99.85%\n",
      "loss: 0.004534  [ 2560/ 2636]  Train Accuracy: 99.85%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.2%, avg loss: 0.001852\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.002301  [    0/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.001168  [  640/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.003491  [ 1280/ 2636]  Train Accuracy: 99.93%\n",
      "loss: 0.005474  [ 1920/ 2636]  Train Accuracy: 99.95%\n",
      "loss: 0.004791  [ 2560/ 2636]  Train Accuracy: 99.92%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.0%, avg loss: 0.001972\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.001573  [    0/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.003435  [  640/ 2636]  Train Accuracy: 99.86%\n",
      "loss: 0.001820  [ 1280/ 2636]  Train Accuracy: 99.93%\n",
      "loss: 0.002761  [ 1920/ 2636]  Train Accuracy: 99.95%\n",
      "loss: 0.000669  [ 2560/ 2636]  Train Accuracy: 99.92%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.2%, avg loss: 0.002049\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.001350  [    0/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.001343  [  640/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.000620  [ 1280/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.026870  [ 1920/ 2636]  Train Accuracy: 99.90%\n",
      "loss: 0.000627  [ 2560/ 2636]  Train Accuracy: 99.89%\n",
      "\n",
      "Test Error:\n",
      "acc: 97.9%, avg loss: 0.002564\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.000495  [    0/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.009018  [  640/ 2636]  Train Accuracy: 99.15%\n",
      "loss: 0.002744  [ 1280/ 2636]  Train Accuracy: 99.11%\n",
      "loss: 0.001699  [ 1920/ 2636]  Train Accuracy: 99.14%\n",
      "loss: 0.016600  [ 2560/ 2636]  Train Accuracy: 99.20%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.0%, avg loss: 0.001913\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.000536  [    0/ 2636]  Train Accuracy: 100.00%\n",
      "loss: 0.006038  [  640/ 2636]  Train Accuracy: 99.72%\n",
      "loss: 0.006185  [ 1280/ 2636]  Train Accuracy: 99.85%\n",
      "loss: 0.001902  [ 1920/ 2636]  Train Accuracy: 99.85%\n",
      "loss: 0.001413  [ 2560/ 2636]  Train Accuracy: 99.70%\n",
      "\n",
      "Test Error:\n",
      "acc: 98.0%, avg loss: 0.002222\n",
      "\n",
      "Early stopping triggered. No improvement in test loss for 10 epochs.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Define the early stopping parameters\n",
    "early_stopping_patience = 10  # Number of epochs to wait before early stopping\n",
    "best_loss = torch.inf\n",
    "wait = 0  # Counter for patience\n",
    "epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    train(train_dataloader, model, cost, optimizer)\n",
    "    test_loss = test(test_dataloader, model)\n",
    "\n",
    "    # Check if the test loss has improved\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        wait = 0  # Reset patience\n",
    "    else:\n",
    "        wait += 1\n",
    "\n",
    "    if wait >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered. No improvement in test loss for {} epochs.\".format(early_stopping_patience))\n",
    "        break  # Stop training\n",
    "\n",
    "print('Done!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:03:01.701976900Z",
     "start_time": "2023-10-13T06:57:05.383499700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as Resnet34_Model_2023-10-13--15-03-09.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current timestamp in the desired format\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "# Define the file name with the timestamp\n",
    "file_name = f\"Resnet34_Model_{timestamp}.pt\"\n",
    "\n",
    "# Save the entire model (including architecture and weights)\n",
    "torch.save(model, file_name)\n",
    "\n",
    "# Print the saved file name\n",
    "print(f\"Model saved as {file_name}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:03:10.112071Z",
     "start_time": "2023-10-13T07:03:10.028720300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Load the model's state_dict\n",
    "model = torch.load('Resnet34_Model_2023-10-13--14-03-45.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:56:58.453997600Z",
     "start_time": "2023-10-13T06:56:58.269655900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 64, 32, 431]           9,408\n",
      "       BatchNorm2d-2          [-1, 64, 32, 431]             128\n",
      "              ReLU-3          [-1, 64, 32, 431]               0\n",
      "         MaxPool2d-4          [-1, 64, 16, 216]               0\n",
      "            Conv2d-5          [-1, 64, 16, 216]          36,864\n",
      "       BatchNorm2d-6          [-1, 64, 16, 216]             128\n",
      "              ReLU-7          [-1, 64, 16, 216]               0\n",
      "            Conv2d-8          [-1, 64, 16, 216]          36,864\n",
      "       BatchNorm2d-9          [-1, 64, 16, 216]             128\n",
      "             ReLU-10          [-1, 64, 16, 216]               0\n",
      "       BasicBlock-11          [-1, 64, 16, 216]               0\n",
      "           Conv2d-12          [-1, 64, 16, 216]          36,864\n",
      "      BatchNorm2d-13          [-1, 64, 16, 216]             128\n",
      "             ReLU-14          [-1, 64, 16, 216]               0\n",
      "           Conv2d-15          [-1, 64, 16, 216]          36,864\n",
      "      BatchNorm2d-16          [-1, 64, 16, 216]             128\n",
      "             ReLU-17          [-1, 64, 16, 216]               0\n",
      "       BasicBlock-18          [-1, 64, 16, 216]               0\n",
      "           Conv2d-19          [-1, 64, 16, 216]          36,864\n",
      "      BatchNorm2d-20          [-1, 64, 16, 216]             128\n",
      "             ReLU-21          [-1, 64, 16, 216]               0\n",
      "           Conv2d-22          [-1, 64, 16, 216]          36,864\n",
      "      BatchNorm2d-23          [-1, 64, 16, 216]             128\n",
      "             ReLU-24          [-1, 64, 16, 216]               0\n",
      "       BasicBlock-25          [-1, 64, 16, 216]               0\n",
      "           Conv2d-26          [-1, 128, 8, 108]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 8, 108]             256\n",
      "             ReLU-28          [-1, 128, 8, 108]               0\n",
      "           Conv2d-29          [-1, 128, 8, 108]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 8, 108]             256\n",
      "           Conv2d-31          [-1, 128, 8, 108]           8,192\n",
      "      BatchNorm2d-32          [-1, 128, 8, 108]             256\n",
      "             ReLU-33          [-1, 128, 8, 108]               0\n",
      "       BasicBlock-34          [-1, 128, 8, 108]               0\n",
      "           Conv2d-35          [-1, 128, 8, 108]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 8, 108]             256\n",
      "             ReLU-37          [-1, 128, 8, 108]               0\n",
      "           Conv2d-38          [-1, 128, 8, 108]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 8, 108]             256\n",
      "             ReLU-40          [-1, 128, 8, 108]               0\n",
      "       BasicBlock-41          [-1, 128, 8, 108]               0\n",
      "           Conv2d-42          [-1, 128, 8, 108]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 8, 108]             256\n",
      "             ReLU-44          [-1, 128, 8, 108]               0\n",
      "           Conv2d-45          [-1, 128, 8, 108]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 8, 108]             256\n",
      "             ReLU-47          [-1, 128, 8, 108]               0\n",
      "       BasicBlock-48          [-1, 128, 8, 108]               0\n",
      "           Conv2d-49          [-1, 128, 8, 108]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 8, 108]             256\n",
      "             ReLU-51          [-1, 128, 8, 108]               0\n",
      "           Conv2d-52          [-1, 128, 8, 108]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 8, 108]             256\n",
      "             ReLU-54          [-1, 128, 8, 108]               0\n",
      "       BasicBlock-55          [-1, 128, 8, 108]               0\n",
      "           Conv2d-56           [-1, 256, 4, 54]         294,912\n",
      "      BatchNorm2d-57           [-1, 256, 4, 54]             512\n",
      "             ReLU-58           [-1, 256, 4, 54]               0\n",
      "           Conv2d-59           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-60           [-1, 256, 4, 54]             512\n",
      "           Conv2d-61           [-1, 256, 4, 54]          32,768\n",
      "      BatchNorm2d-62           [-1, 256, 4, 54]             512\n",
      "             ReLU-63           [-1, 256, 4, 54]               0\n",
      "       BasicBlock-64           [-1, 256, 4, 54]               0\n",
      "           Conv2d-65           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-66           [-1, 256, 4, 54]             512\n",
      "             ReLU-67           [-1, 256, 4, 54]               0\n",
      "           Conv2d-68           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-69           [-1, 256, 4, 54]             512\n",
      "             ReLU-70           [-1, 256, 4, 54]               0\n",
      "       BasicBlock-71           [-1, 256, 4, 54]               0\n",
      "           Conv2d-72           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-73           [-1, 256, 4, 54]             512\n",
      "             ReLU-74           [-1, 256, 4, 54]               0\n",
      "           Conv2d-75           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-76           [-1, 256, 4, 54]             512\n",
      "             ReLU-77           [-1, 256, 4, 54]               0\n",
      "       BasicBlock-78           [-1, 256, 4, 54]               0\n",
      "           Conv2d-79           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-80           [-1, 256, 4, 54]             512\n",
      "             ReLU-81           [-1, 256, 4, 54]               0\n",
      "           Conv2d-82           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-83           [-1, 256, 4, 54]             512\n",
      "             ReLU-84           [-1, 256, 4, 54]               0\n",
      "       BasicBlock-85           [-1, 256, 4, 54]               0\n",
      "           Conv2d-86           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-87           [-1, 256, 4, 54]             512\n",
      "             ReLU-88           [-1, 256, 4, 54]               0\n",
      "           Conv2d-89           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-90           [-1, 256, 4, 54]             512\n",
      "             ReLU-91           [-1, 256, 4, 54]               0\n",
      "       BasicBlock-92           [-1, 256, 4, 54]               0\n",
      "           Conv2d-93           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-94           [-1, 256, 4, 54]             512\n",
      "             ReLU-95           [-1, 256, 4, 54]               0\n",
      "           Conv2d-96           [-1, 256, 4, 54]         589,824\n",
      "      BatchNorm2d-97           [-1, 256, 4, 54]             512\n",
      "             ReLU-98           [-1, 256, 4, 54]               0\n",
      "       BasicBlock-99           [-1, 256, 4, 54]               0\n",
      "          Conv2d-100           [-1, 512, 2, 27]       1,179,648\n",
      "     BatchNorm2d-101           [-1, 512, 2, 27]           1,024\n",
      "            ReLU-102           [-1, 512, 2, 27]               0\n",
      "          Conv2d-103           [-1, 512, 2, 27]       2,359,296\n",
      "     BatchNorm2d-104           [-1, 512, 2, 27]           1,024\n",
      "          Conv2d-105           [-1, 512, 2, 27]         131,072\n",
      "     BatchNorm2d-106           [-1, 512, 2, 27]           1,024\n",
      "            ReLU-107           [-1, 512, 2, 27]               0\n",
      "      BasicBlock-108           [-1, 512, 2, 27]               0\n",
      "          Conv2d-109           [-1, 512, 2, 27]       2,359,296\n",
      "     BatchNorm2d-110           [-1, 512, 2, 27]           1,024\n",
      "            ReLU-111           [-1, 512, 2, 27]               0\n",
      "          Conv2d-112           [-1, 512, 2, 27]       2,359,296\n",
      "     BatchNorm2d-113           [-1, 512, 2, 27]           1,024\n",
      "            ReLU-114           [-1, 512, 2, 27]               0\n",
      "      BasicBlock-115           [-1, 512, 2, 27]               0\n",
      "          Conv2d-116           [-1, 512, 2, 27]       2,359,296\n",
      "     BatchNorm2d-117           [-1, 512, 2, 27]           1,024\n",
      "            ReLU-118           [-1, 512, 2, 27]               0\n",
      "          Conv2d-119           [-1, 512, 2, 27]       2,359,296\n",
      "     BatchNorm2d-120           [-1, 512, 2, 27]           1,024\n",
      "            ReLU-121           [-1, 512, 2, 27]               0\n",
      "      BasicBlock-122           [-1, 512, 2, 27]               0\n",
      "AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n",
      "          Linear-124                    [-1, 2]           1,026\n",
      "================================================================\n",
      "Total params: 21,285,698\n",
      "Trainable params: 21,285,698\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.63\n",
      "Forward/backward pass size (MB): 106.06\n",
      "Params size (MB): 81.20\n",
      "Estimated Total Size (MB): 187.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Assuming 'model' is your PyTorch model\n",
    "summary(model, input_size=(3, 64, 862))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:03:15.997563400Z",
     "start_time": "2023-10-13T07:03:15.906195300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pad_waveform(waveform, target_length):\n",
    "    num_channels, current_length = waveform.shape\n",
    "\n",
    "    if current_length < target_length:\n",
    "        # Calculate the amount of padding needed\n",
    "        padding = target_length - current_length\n",
    "        # Pad the waveform with zeros on the right side\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "\n",
    "    return waveform\n",
    "\n",
    "# Define a function to transform audio data into images\n",
    "def transform_data_to_image(audio, sample_rate, label, i):\n",
    "    # Pad waveform to a consistent length of 44100 samples\n",
    "    audio = pad_waveform(audio, 441000)\n",
    "\n",
    "    spectrogram_tensor = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=64, n_fft=1024)(audio)[0] + 1e-10\n",
    "\n",
    "    # Save the spectrogram as an image\n",
    "    image_path = f'Data/TestImages/{label}/image{i}.png'\n",
    "\n",
    "    plt.imsave(image_path, spectrogram_tensor.log2().numpy(), cmap='viridis')\n",
    "    return image_path\n",
    "\n",
    "# Define the image transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 862)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x[:3, :, :])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:20:06.450958700Z",
     "start_time": "2023-10-13T07:20:06.443891200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "                Filename  Prediction\n0    ---1_cCGK4M_out.wav           0\n1    -20uudT97E0_out.wav           1\n2    -2yygHLdpXc_out.wav           1\n3    -3bGlOhRkAo_out.wav           1\n4    -4pUrlMafww_out.wav           1\n..                   ...         ...\n857  _QMEw67gWIA_out.wav           1\n858  _TLzbbay6Hw_out.wav           1\n859  _XPPISqmXSE_out.wav           1\n860  _xRpsu02t9o_out.wav           1\n861  _zzhHu7HwZc_out.wav           1\n\n[862 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>---1_cCGK4M_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-20uudT97E0_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-2yygHLdpXc_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-3bGlOhRkAo_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-4pUrlMafww_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>857</th>\n      <td>_QMEw67gWIA_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>858</th>\n      <td>_TLzbbay6Hw_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>859</th>\n      <td>_XPPISqmXSE_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>_xRpsu02t9o_out.wav</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>861</th>\n      <td>_zzhHu7HwZc_out.wav</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>862 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing WAV files\n",
    "folder_path = 'Data/Screaming'  # Replace with the path to your folder\n",
    "label = 'Screaming'  # Label for the images\n",
    "\n",
    "# Create an empty list to store data\n",
    "predictions_data = []\n",
    "\n",
    "# Iterate through WAV files in the folder\n",
    "for i, filename in enumerate(os.listdir(folder_path)):\n",
    "    if filename.endswith('.wav'):\n",
    "        # Load the audio\n",
    "        audio, sample_rate = torchaudio.load(os.path.join(folder_path, filename))\n",
    "\n",
    "        # Transform audio to an image and save it\n",
    "        image_path = transform_data_to_image(audio, sample_rate, label, i)\n",
    "\n",
    "        # Load the saved image and apply transformations\n",
    "        image = Image.open(image_path)\n",
    "        image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Make predictions using the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image.to(device))\n",
    "\n",
    "        predict = outputs.argmax(dim=1).cpu().detach().numpy().ravel()[0]\n",
    "\n",
    "        # Store the filename and prediction in the DataFrame\n",
    "        predictions_data.append({'Filename': filename, 'Prediction': predict})\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "scream_predictions_df = pd.DataFrame(predictions_data)\n",
    "\n",
    "# Display the DataFrame with predictions\n",
    "scream_predictions_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:26:46.881052Z",
     "start_time": "2023-10-13T07:25:52.473559200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "Prediction\n1    758\n0    104\nName: count, dtype: int64"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scream_predictions_df['Prediction'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:26:52.333400200Z",
     "start_time": "2023-10-13T07:26:52.320400100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "                 Filename  Prediction\n0     --PJHxphWEs_out.wav           0\n1     -28U1_qW0sU_out.wav           0\n2     -4xJv59_zcA_out.wav           0\n3     -5GhUbDLYkQ_out.wav           0\n4     -5Jlimvsuwo_out.wav           0\n...                   ...         ...\n2626  _XusTa2prSw_out.wav           0\n2627  _y07ENAx2_E_out.wav           0\n2628  _yqlQimkHpQ_out.wav           0\n2629  _Zsk5Fxqbkc_out.wav           0\n2630  __qxgIqI0uA_out.wav           0\n\n[2631 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>--PJHxphWEs_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-28U1_qW0sU_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-4xJv59_zcA_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-5GhUbDLYkQ_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-5Jlimvsuwo_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2626</th>\n      <td>_XusTa2prSw_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2627</th>\n      <td>_y07ENAx2_E_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2628</th>\n      <td>_yqlQimkHpQ_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2629</th>\n      <td>_Zsk5Fxqbkc_out.wav</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2630</th>\n      <td>__qxgIqI0uA_out.wav</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2631 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the folder containing WAV files\n",
    "folder_path = 'Data/NotScreaming'  # Replace with the path to your folder\n",
    "label = 'NotScreaming'  # Label for the images\n",
    "import pandas as pd\n",
    "\n",
    "# Create an empty list to store data\n",
    "predictions_data = []\n",
    "\n",
    "# Iterate through WAV files in the folder\n",
    "for i, filename in enumerate(os.listdir(folder_path)):\n",
    "    if filename.endswith('.wav'):\n",
    "        # Load the audio\n",
    "        audio, sample_rate = torchaudio.load(os.path.join(folder_path, filename))\n",
    "\n",
    "        # Transform audio to an image and save it\n",
    "        image_path = transform_data_to_image(audio, sample_rate, label, i)\n",
    "\n",
    "        # Load the saved image and apply transformations\n",
    "        image = Image.open(image_path)\n",
    "        image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Make predictions using the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image.to(device))\n",
    "\n",
    "        predict = outputs.argmax(dim=1).cpu().detach().numpy().ravel()[0]\n",
    "\n",
    "        # Store the filename and prediction in the DataFrame\n",
    "        predictions_data.append({'Filename': filename, 'Prediction': predict})\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "not_scream_predictions_df = pd.DataFrame(predictions_data)\n",
    "\n",
    "# Display the DataFrame with predictions\n",
    "not_scream_predictions_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:29:44.854404800Z",
     "start_time": "2023-10-13T07:26:55.236564800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "Prediction\n0    2585\n1      46\nName: count, dtype: int64"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_scream_predictions_df['Prediction'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:29:56.258558100Z",
     "start_time": "2023-10-13T07:29:56.252516500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# Compute TP, FP, TN, FN for Scream predictions\n",
    "scream_TP = scream_predictions_df['Prediction'].value_counts().get(1, 0)\n",
    "scream_FN = scream_predictions_df['Prediction'].value_counts().get(0, 0)\n",
    "scream_samples = len(scream_predictions_df)\n",
    "\n",
    "# Compute TP, FP, TN, FN for Not Scream predictions\n",
    "scream_TN = not_scream_predictions_df['Prediction'].value_counts().get(0, 0)\n",
    "scream_FP = not_scream_predictions_df['Prediction'].value_counts().get(1, 0)\n",
    "not_scream_samples = len(not_scream_predictions_df)\n",
    "\n",
    "# Calculate total samples\n",
    "total_samples = scream_samples + not_scream_samples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:42:11.837113700Z",
     "start_time": "2023-10-13T07:42:11.829588700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "Precision: 0.94\n",
      "Recall: 0.88\n",
      "F1 Score: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = (scream_TP + scream_TN) / total_samples\n",
    "\n",
    "# Calculate precision\n",
    "precision = scream_TP / (scream_TP + scream_FP)\n",
    "\n",
    "# Calculate recall\n",
    "recall = scream_TP / (scream_TP + scream_FN)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 Score: {:.2f}\".format(f1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:45:18.593620500Z",
     "start_time": "2023-10-13T07:45:18.584391700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
